{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AulaBonus.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5gS8ydHhYxi",
        "colab_type": "text"
      },
      "source": [
        "#Introdução à inteligência artificial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCEF_z9lf8hK",
        "colab_type": "text"
      },
      "source": [
        "## Transferência de estilo com redes neurais profundas\n",
        "\n",
        "\n",
        "Nesta aula, recriaremos um método de transferência de estilo descrito no artigo [Transferência de estilo de imagem usando redes neurais convolucionais, por Gatys](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) no PyTorch.\n",
        "\n",
        "Neste artigo, a transferência de estilo usa os recursos encontrados na Rede VGG de 19 camadas, que é composta por uma série de camadas convolucionais e de pool e algumas camadas totalmente conectadas. Na imagem abaixo, as camadas convolucionais são nomeadas por pilha e sua ordem na pilha. Conv_1_1 é a primeira camada convolucional pela qual uma imagem é passada, na primeira pilha. Conv_2_1 é a primeira camada convolucional na *segunda* pilha . A camada convolucional mais profunda da rede é conv_5_4.\n",
        "\n",
        "![](https://miro.medium.com/max/908/1*PrmF6fhniOOGK8WImxM2KQ.png)\n",
        "\n",
        "## Separando estilo e conteúdo\n",
        "\n",
        "A transferência de estilos depende da separação do conteúdo e do estilo de uma imagem. Dada uma imagem de conteúdo e uma imagem de estilo, pretendemos criar uma nova imagem _target_ que deve conter o conteúdo desejado e os componentes de estilo:\n",
        "* os objetos e sua organização são semelhantes aos da **imagem de conteúdo**\n",
        "* estilo, cores e texturas são semelhantes às da **imagem de estilo**\n",
        "\n",
        "Um exemplo é mostrado abaixo, onde a imagem de conteúdo é de um gato e a imagem de estilo é de [Grande Onda de Hokusai](https://pt.wikipedia.org/wiki/A_Grande_Onda_de_Kanagawa). A imagem de destino gerada ainda contém o gato, mas é estilizada com as ondas, cores azul e bege e texturas de impressão em bloco da imagem de estilo!\n",
        "\n",
        "![](https://miro.medium.com/max/1202/1*1A15ZgDnOn_64myq4rxxkg.png)\n",
        "\n",
        "Nesta aula, usaremos uma rede VGG19 pré-treinada para extrair conteúdo ou recursos de estilo de uma imagem passada. Formalizaremos a ideia de conteúdo e _perdas_ de estilo e as usaremos para atualizar iterativamente nossa imagem de destino até obtermos o resultado desejado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyI3md-RgfPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importação de bibliotecas\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXjdfdO_iN5T",
        "colab_type": "text"
      },
      "source": [
        "## Carregar no VGG19 (features)\n",
        "\n",
        "O VGG19 é dividido em duas partes:\n",
        "* `vgg19.features`, que são todas as camadas convolucionais e de pool\n",
        "* `vgg19.classifier`, que são as três camadas classificadoras lineares no final\n",
        "\n",
        "Nós só precisamos da parte `features`, que vamos carregar e \"congelar\" os pesos abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBhuuqU3h_dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obter a parte \"features\" do VGG19 (não precisaremos da parte \"classifier\")\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# Congelar todos os parâmetros do VGG, pois estamos apenas otimizando a imagem de destino\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsFi0fy3imyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move o modelo para GPU, se disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vgg.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbEydfAMiyaC",
        "colab_type": "text"
      },
      "source": [
        "## Carregar imagens de conteúdo e estilo\n",
        "\n",
        "Você pode carregar as imagens que desejar! Abaixo, haverá uma função auxiliar para carregar qualquer tipo e tamanho de imagem. A função `load_image` também converte imagens em tensores normalizados.\n",
        "\n",
        "Além disso, será mais fácil ter imagens menores e compactar as imagens de conteúdo e estilo para que elas tenham o mesmo tamanho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSCo49Aqity_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(img_path, max_size=400, shape=None):\n",
        "    ''' Carrega e transforma uma imagem, certificando-se de \n",
        "    que a imagem tenha <= 400 pixels nas dimensões x-y.'''\n",
        "    \n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    # Imagens grandes deixarão o processamento devagar\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "    \n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "        \n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # Descarte o canal alfa transparente (que é o 3) e adicione a dimensão do lote\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "    \n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSr23BcBjUZJ",
        "colab_type": "text"
      },
      "source": [
        "Em seguida, carregue as imagens pelo nome do arquivo e força a imagem do estilo para o mesmo tamanho da imagem de conteúdo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEqc3tJsjsCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Baixar imagens de exemplo\n",
        "!wget -O hokusai_wave.jpg https://upload.wikimedia.org/wikipedia/commons/b/b3/Katsushika_Hokusai_-_Thirty-Six_Views_of_Mount_Fuji-_The_Great_Wave_Off_the_Coast_of_Kanagawa_-_Google_Art_Project.jpg\n",
        "!wget -O dog.jpg https://love.doghero.com.br/wp-content/uploads/2018/12/golden-retriever-1.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrmdBACyjQOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carregar imagem de conteúdo e estilo\n",
        "content = load_image('/content/dog.jpg').to(device)\n",
        "# Redimensionar o estilo para combinar com o conteúdo, facilita o código\n",
        "style = load_image('/content/hokusai_wave.jpg', shape=content.shape[-2:]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4f3qXlnlpAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função auxiliar para desnormalizar uma imagem\n",
        "# e converter de uma imagem Tensor para imagem Numpy para mostra-la\n",
        "def im_convert(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "    \n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwzzWrBmAp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mostra as imagens\n",
        "plt.figure()\n",
        "plt.imshow(im_convert(content))\n",
        "plt.figure()\n",
        "plt.imshow(im_convert(style))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qab_Yz9CmpNp",
        "colab_type": "text"
      },
      "source": [
        "## Camadas VGG19\n",
        "\n",
        "Para obter as representações de conteúdo e estilo de uma imagem, precisamos passar uma imagem adiante pela rede VGG19 até chegar à(s) camada(s) desejada(s) e depois obter a saída dessa camada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO9fKngumB17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imprime a estrutura VGG19 para poder ver os nomes de várias camadas\n",
        "vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLYksSslnJde",
        "colab_type": "text"
      },
      "source": [
        "## Recursos de conteúdo e estilo\n",
        "\n",
        "Abaixo, função para mapeamento dos nomes das camadas para os nomes encontrados no \"paper\" para a _respresentação do conteúdo_ e a _representação de estilo_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWM9ZDEgm078",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    \"\"\" Executa uma imagem adiante através de um modelo e obtenha os recursos para um conjunto de camadas. \n",
        "    Camadas padrão são para correspondência VGGNet Gatys et al (2016)\n",
        "    \"\"\"\n",
        "\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1', \n",
        "                  '10': 'conv3_1', \n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2', \n",
        "                  '28': 'conv5_1'}\n",
        "        \n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "            \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wIfLoRlnvzJ",
        "colab_type": "text"
      },
      "source": [
        "## Matriz Gram \n",
        "\n",
        "A saída de cada camada convolucional é um tensor com dimensões associadas ao `batch_size`, uma profundidade, `d` e alguma altura e largura (`h`,`w`). A matriz Gram de uma camada convolucional pode ser calculada da seguinte forma:\n",
        "* Obtenha a profundidade, altura e largura de um tensor usando `batch_size, d, h, w = tensor.size`\n",
        "* Remodelar esse tensor para que as dimensões espaciais sejam achatadas\n",
        "* Calcule a matriz grama multiplicando o tensor remodelado pela sua transposição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozWmPPxEn3qU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "    \"\"\" Calcular a matriz de Gram de um dado tensor\n",
        "    Matriz de Gram: https://en.wikipedia.org/wiki/Gramian_matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    _, d, h, w = tensor.size()\n",
        "    \n",
        "    tensor = tensor.view(d, h * w)\n",
        "    \n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    \n",
        "    return gram "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GT9J_cfoPzO",
        "colab_type": "text"
      },
      "source": [
        "## Juntando tudo\n",
        "\n",
        "Agora que escrevemos funções para extrair recursos e calcular a matriz Gram de uma determinada camada convolucional; vamos juntar todas essas peças! Extrairemos nossos recursos de nossas imagens e calcularemos as matrizes de grama para cada camada em nossa representação de estilo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2j2xdgCoT64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obtenha recursos de conteúdo e estilo apenas uma vez antes do treinamento\n",
        "content_features = get_features(content, vgg)\n",
        "style_features = get_features(style, vgg)\n",
        "\n",
        "# Calcular as matrizes de Gram para cada camada de nossa representação de estilo\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# Criar uma terceira imagem \"alvo\" e prepará-la para alterações\n",
        "# é uma boa ideia começar com o destino como uma cópia da nossa imagem *content*\n",
        "# e iterativamente mudar seu estilo\n",
        "target = content.clone().requires_grad_(True).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nwN-MBNsZaZ",
        "colab_type": "text"
      },
      "source": [
        "## Perdas e pesos\n",
        "\n",
        "#### Pesos de estilos de camadas individuais\n",
        "\n",
        "Abaixo, você tem a opção de ponderar a representação do estilo em cada camada relevante. Sugerimos que você use um intervalo entre 0 e 1 para ponderar essas camadas. Ao ponderar as camadas anteriores (`conv1_1` e` conv2_1`) mais, é possível obter artefatos mais do estilo na imagem de destino resultante. Se você optar por ponderar as camadas posteriores, terá mais ênfase em recursos menores. Isso ocorre porque cada camada tem um tamanho diferente e, juntos, eles criam uma representação de estilo em várias escalas!\n",
        "\n",
        "#### Peso do conteúdo e estilo\n",
        "\n",
        "Assim como no artigo, definimos um alfa (`content_weight`) e um beta (`style_weight`). Essa proporção afetará o quão estilizada é sua imagem final. É recomendável deixar o content_weight = 1 e definir o style_weight para atingir a proporção desejada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z94hgQAyovti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pesos para cada camada de estilo\n",
        "# Ponderar mais camadas anteriores resultará em artefatos de estilo *maiores*\n",
        "# Observe que estamos excluindo 'conv4_2' nossa representação de conteúdo\n",
        "style_weights = {'conv1_1': 1.,\n",
        "                 'conv2_1': 0.8,\n",
        "                 'conv3_1': 0.5,\n",
        "                 'conv4_1': 0.3,\n",
        "                 'conv5_1': 0.1}\n",
        "\n",
        "# Você pode optar por deixá-los como estão\n",
        "content_weight = 1  # alpha\n",
        "style_weight = 1e6  # beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBIQ9WK8sbuz",
        "colab_type": "text"
      },
      "source": [
        "## Atualizando a meta e calculando perdas\n",
        "\n",
        "Você decidirá sobre várias etapas para atualizar sua imagem; isso é semelhante ao ciclo de treinamento que você já viu antes, apenas estamos alterando nossa imagem _target_ e nada mais sobre o VGG19 ou qualquer outra imagem. Portanto, você decide realmente o número de etapas! **Recomendo usar pelo menos 2000 etapas para obter bons resultados.** Mas você pode começar com menos etapas se estiver apenas testando diferentes valores de peso ou experimentando imagens diferentes.\n",
        "\n",
        "Dentro do loop de iteração, você calculará as perdas de conteúdo e estilo e atualizará sua imagem de destino, de acordo.\n",
        "\n",
        "#### Perda de conteúdo\n",
        "\n",
        "A perda de conteúdo será a diferença quadrática média entre os recursos de destino e conteúdo na camada `conv4_2`. Isso pode ser calculado da seguinte maneira:\n",
        "```\n",
        "content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "```\n",
        "\n",
        "#### Perda de estilo\n",
        "\n",
        "A perda de estilo é calculada de maneira semelhante, apenas você precisa percorrer um número de camadas, especificadas pelo nome no nosso dicionário `style_weights`.\n",
        "> Você calculará a matriz de Gram para a imagem de destino, `target_gram` e a imagem de estilo` style_gram` em cada uma dessas camadas e comparará essas matrizes de grama, calculando a `layer_style_loss`.\n",
        "> Mais tarde, você verá que esse valor é normalizado pelo tamanho da camada.\n",
        "\n",
        "#### Perda total\n",
        "\n",
        "Por fim, você criará a perda total adicionando as perdas de estilo e conteúdo e ponderando-as com o alfa e o beta especificados!\n",
        "\n",
        "Intermitentemente, imprimiremos essa perda; não se assuste se a perda for muito grande. Leva algum tempo para que o estilo de uma imagem mude e você deve se concentrar na aparência da imagem de destino e não em qualquer valor de perda. Ainda assim, você deve ver que essa perda diminui com o número de iterações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCvVYofrRLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Para exibir a imagem de destino, de forma intermitente\n",
        "show_every = 400\n",
        "\n",
        "# hyperparâmetros\n",
        "optimizer = optim.Adam([target], lr=0.003)\n",
        "steps = 5000  # Decida quantas iterações\n",
        "\n",
        "for ii in range(1, steps+1):\n",
        "    \n",
        "    target_features = get_features(target, vgg)\n",
        "    \n",
        "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "\n",
        "    style_loss = 0\n",
        "\n",
        "    for layer in style_weights:\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "        style_gram = style_grams[layer]\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "        \n",
        "        \n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if  ii % show_every == 0:\n",
        "        print('Total loss: ', total_loss.item())\n",
        "        plt.imshow(im_convert(target))\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0EvfKHPseSH",
        "colab_type": "text"
      },
      "source": [
        "## Mostra o resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WOSWuFmrTnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mostra a imagem de conteudo e o resultado\n",
        "plt.figure()\n",
        "plt.imshow(im_convert(content))\n",
        "plt.figure()\n",
        "plt.imshow(im_convert(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkfsjcS6Qypg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exportar para imagem .jpg\n",
        "matplotlib.image.imsave('/content/result.jpg',im_convert(target))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}